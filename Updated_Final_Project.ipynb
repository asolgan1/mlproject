{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated_Final_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Presentation: Sentiment Prediction\n",
        "Matthew Landry, Alexey Solganik, Michael Klisiwecz"
      ],
      "metadata": {
        "id": "aYgfWXbzzgMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing\n",
        "Our project is sentiment prediction on sentiment labelled reviews from yelp, imdb, and amazon. Before we can create a model to predict sentiment, we need to preprocess the sentences from the example data set so they can be represented as vectors. First, we remove unnecessary symbols from the sentences (punctuation, numbers, capital letters, extra spaces) and then tokenize each sentence. Initially, we tried to remove stop words as part of our preprocessing step, but it turned out that this would remove words like \"not\", which change the sentiment of a sentence, so we realized we could not include this step even if it means leaving in many words that do not affect sentence sentiment (ex. a, the, is). We used PorterStemmer from the gensim library to remove word stems, because we want as many instances as possible of a given word in our training data, so it is best to consider all forms of a word (ex. large, larger, largest) as the same word. "
      ],
      "metadata": {
        "id": "R5Mrm4VGttRJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Sn87uqZnpvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d67e4a-6e53-4f00-90a2-08eff02448ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Phrases  Labels\n",
            "0  [very, very, very, slow, moving, aimless, movi...       0\n",
            "1  [not, sure, who, more, lost, flat, characters,...       0\n",
            "2  [attempting, artiness, with, black, white, cle...       0\n",
            "3         [very, little, music, anything, speak, of]       0\n",
            "4  [best, scene, movie, when, gerardo, trying, fi...       1\n",
            "5  [rest, of, movie, lacks, art, charm, meaning, ...       0\n",
            "6                               [wasted, two, hours]       0\n",
            "7  [saw, movie, today, thought, it, good, effort,...       1\n",
            "8                                 [bit, predictable]       0\n",
            "9  [loved, casting, of, jimmy, buffet, as, scienc...       1\n",
            "                                             Phrases  Labels\n",
            "0  [so, there, no, way, for, me, plug, it, here, ...       0\n",
            "1                     [good, case, excellent, value]       1\n",
            "2                              [great, for, jawbone]       1\n",
            "3  [tied, charger, for, conversations, lasting, m...       0\n",
            "4                                       [mic, great]       1\n",
            "5  [i, have, jiggle, plug, get, it, line, up, rig...       0\n",
            "6  [if, you, have, several, dozen, several, hundr...       0\n",
            "7      [if, you, razr, owner, you, must, have, this]       1\n",
            "8              [needless, say, i, wasted, my, money]       0\n",
            "9                     [what, waste, of, money, time]       0\n",
            "                                             Phrases  Labels\n",
            "0                          [wow, loved, this, place]       1\n",
            "1                                 [crust, not, good]       0\n",
            "2                 [not, tasty, texture, just, nasty]       0\n",
            "3  [stopped, by, during, late, may, bank, holiday...       1\n",
            "4               [selection, menu, great, so, prices]       1\n",
            "5  [now, i, am, getting, angry, i, want, my, damn...       0\n",
            "6        [honeslty, it, didn, t, taste, that, fresh]       0\n",
            "7  [potatoes, like, rubber, you, could, tell, the...       0\n",
            "8                                [fries, great, too]       1\n",
            "9                                     [great, touch]       1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    500\n",
              "0    500\n",
              "Name: Labels, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "import gensim\n",
        "import pandas as pd\n",
        "from gensim.parsing.preprocessing import strip_numeric, strip_punctuation, strip_multiple_whitespaces\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "pstem = PorterStemmer()\n",
        "#remove_stopwords(\"Better late than never, but better never late.\")\n",
        "\n",
        "filter = ['a', 'an', 'the', 'is', 'are', 'were', 'was', 'will', 'be', 'in', 'to', 'on','at','and','or']\n",
        "\n",
        "with open(\"imdb_labelled.txt\") as dat:\n",
        "    imdb_data = []\n",
        "    imdb_results = []\n",
        "    for line in dat:\n",
        "        imdb_results.append(int(line[len(line)-2:len(line)-1]))\n",
        "        line = line.lower()\n",
        "        line = strip_punctuation(line)\n",
        "        line = strip_multiple_whitespaces(line)\n",
        "        line = strip_numeric(line)\n",
        "\n",
        "        tokenized_line = line.split()\n",
        "        tokenized_line1 = []\n",
        "        for token in tokenized_line:\n",
        "          if token not in filter:\n",
        "            tokenized_line1.append(token)\n",
        "        tokenized_line = tokenized_line1  \n",
        "       # stem_line = lambda x: [pstem.stem(token) for token in x]\n",
        "        #tokenized_line = stem_line(tokenized_line)\n",
        "\n",
        "        imdb_data.append(tokenized_line)\n",
        "        \n",
        "with open(\"amazon_cells_labelled.txt\") as dat:\n",
        "    amazon_data = []\n",
        "    amazon_results = []\n",
        "    for line in dat:\n",
        "        amazon_results.append(int(line[len(line)-2:len(line)-1]))\n",
        "        line = line.lower()\n",
        "        line = strip_punctuation(line)\n",
        "        line = strip_multiple_whitespaces(line)\n",
        "        line = strip_numeric(line)\n",
        "\n",
        "        tokenized_line = line.split()\n",
        "        tokenized_line1 = []\n",
        "        for token in tokenized_line:\n",
        "          if token not in filter:\n",
        "            tokenized_line1.append(token)\n",
        "        tokenized_line = tokenized_line1 \n",
        "        #stem_line = lambda x: [pstem.stem(token) for token in x]\n",
        "        #tokenized_line = stem_line(tokenized_line)\n",
        "\n",
        "        amazon_data.append(tokenized_line)\n",
        "        \n",
        "with open(\"yelp_labelled.txt\") as dat:\n",
        "    yelp_data = []\n",
        "    yelp_results = []\n",
        "    for line in dat:\n",
        "        yelp_results.append(int(line[len(line)-2:len(line)-1]))\n",
        "        line = line.lower()\n",
        "        line = strip_punctuation(line)\n",
        "        line = strip_multiple_whitespaces(line)\n",
        "        line = strip_numeric(line)\n",
        "\n",
        "        tokenized_line = line.split()\n",
        "        tokenized_line1 = []\n",
        "        for token in tokenized_line:\n",
        "          if token not in filter:\n",
        "            tokenized_line1.append(token)\n",
        "        tokenized_line = tokenized_line1 \n",
        "        #stem_line = lambda x: [pstem.stem(token) for token in x]\n",
        "        #tokenized_line = stem_line(tokenized_line)\n",
        "        \n",
        "        yelp_data.append(tokenized_line)\n",
        "        \n",
        "\n",
        "\n",
        "imdb_df = pd.DataFrame({\"Phrases\" : imdb_data, \"Labels\" : imdb_results})\n",
        "amazon_df = pd.DataFrame({\"Phrases\" : amazon_data, \"Labels\" : amazon_results})\n",
        "yelp_df = pd.DataFrame({\"Phrases\" : yelp_data, \"Labels\" : yelp_results})\n",
        "#imdb_df = imdb_df.reset_index()\n",
        "#imdb_df = imdb_df.rename(columns = {\"index\": \"UID\"})\n",
        "#amazon_df = amazon_df.reset_index()\n",
        "#amazon_df = amazon_df.rename(columns = {\"index\": \"UID\"})\n",
        "#yelp_df = yelp_df.reset_index()\n",
        "#yelp_df = yelp_df.rename(columns = {\"index\": \"UID\"})\n",
        "\n",
        "print(imdb_df[:10])\n",
        "print(amazon_df[:10])\n",
        "print(yelp_df[:10])\n",
        "imdb_df[\"Labels\"].value_counts()\n",
        "amazon_df[\"Labels\"].value_counts()\n",
        "yelp_df[\"Labels\"].value_counts()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.parsing.preprocessing import STOPWORDS\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQzmpFVN8nXm",
        "outputId": "5f406a7e-415c-4cd8-a3e0-a45879117dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frozenset({'ourselves', 'so', 'must', 'now', 'has', 'sixty', 'me', 'who', 'many', 'are', 'above', 'thus', 'these', 'name', 'become', 'may', 'further', 'sometime', 'con', 'everyone', 'your', 'make', 'off', 'no', 'onto', 'in', 'due', 'behind', 'for', 'quite', 'thereby', 'namely', 'wherein', 'somehow', 'since', 'upon', 'because', 'those', 'herein', 'through', 'might', 'out', 'noone', 'whether', 'on', 'give', 'hereby', 'cannot', 'could', 'by', 'whatever', 'per', 'enough', 'indeed', 'of', 'us', 'almost', 'etc', 'therefore', 'still', 'doing', 'also', 'when', 'why', 'de', 'always', 'her', 'all', 'eight', 'whereafter', 'everywhere', 'hereafter', 'they', 'next', 'unless', 'whenever', 'seeming', 'around', 'he', 'sincere', 'much', 'three', 'nowhere', 'to', 'up', 'mine', 'don', 'anyhow', 'otherwise', 'over', 'fifteen', 'thru', 'well', 'nobody', 'seems', 'itself', 'last', 'really', 'top', 'if', 'whose', 'done', 'beside', 'former', 'go', 'six', 'each', 'four', 'my', 'keep', 'towards', 'down', 'was', 'bill', 'detail', 'besides', 'something', 'interest', 'some', 'thereupon', 'thin', 'call', 'latterly', 'than', 'is', 'find', 'then', 'eleven', 'cant', 'via', 'should', 'hasnt', 'else', 'throughout', 'here', 'hundred', 'before', 'ours', 'too', 'therein', 'two', 'front', 'their', 'were', 'yourself', 'among', 'made', 'somewhere', 'becoming', 'yours', 'nevertheless', 'themselves', 'found', 'anyone', 'do', 'once', 'have', 'that', 'couldnt', 'across', 'hereupon', 'move', 'became', 'an', 'third', 'seem', 'i', 'amoungst', 'whole', 'except', 'beforehand', 'becomes', 'see', 'very', 'formerly', 'mill', 'we', 'computer', 'however', 'back', 'be', 'hers', 'amount', 'system', 'there', 'just', 'mostly', 'thick', 'against', 'wherever', 'anything', 'seemed', 'regarding', 'say', 'being', 'fill', 'please', 'a', 'amongst', 'sometimes', 'ie', 'co', 'same', 'which', 'doesn', 'less', 'rather', 'meanwhile', 'nor', 'the', 'five', 'someone', 'least', 'moreover', 'thence', 'can', 'ten', 'others', 'twelve', 'whereupon', 'not', 'inc', 'this', 'whom', 'nothing', 'himself', 'even', 'cry', 'few', 'until', 'used', 'about', 'you', 'forty', 'afterwards', 'into', 'had', 'km', 'his', 'other', 'describe', 'own', 'both', 'from', 'toward', 'whereas', 'nine', 'either', 'between', 'but', 'yet', 'more', 'one', 'him', 'such', 'latter', 'myself', 'does', 'would', 'did', 'alone', 'take', 'under', 'beyond', 'several', 'below', 'its', 'empty', 'together', 'whither', 'how', 'herself', 'she', 'put', 'will', 'whereby', 'ever', 'within', 'during', 'get', 'un', 'anyway', 'first', 'at', 'without', 'with', 'anywhere', 'whence', 'eg', 'been', 'part', 'hence', 'although', 'most', 'where', 'again', 'along', 'after', 'kg', 'elsewhere', 'everything', 'and', 'thereafter', 'twenty', 'using', 'what', 'while', 'none', 'ltd', 'or', 'bottom', 'as', 'our', 'often', 're', 'perhaps', 'full', 'them', 'yourselves', 'fire', 'any', 'fify', 'though', 'already', 'neither', 'another', 'various', 'it', 'only', 'serious', 'side', 'whoever', 'every', 'show', 'didn', 'never', 'am'})\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above output shows examples of tokenized sentences from our example dataset of user reviews from imdb, amazon, and yelp. Each sentence has been tokenized and stripped of unneccessary symbols. "
      ],
      "metadata": {
        "id": "WpQuMYDNvUSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"IMDB_data.txt\", sep='\\t')"
      ],
      "metadata": {
        "id": "oRjW7ERDhXjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[:5])\n",
        "tokenized_lines = []\n",
        "for line in data[\"review\"]:\n",
        "  line = line.lower()\n",
        "  line = strip_punctuation(line)\n",
        "  line = strip_multiple_whitespaces(line)\n",
        "  line = strip_numeric(line)\n",
        "\n",
        "  tokenized_lines.append(line.split())\n",
        "\n",
        "tokenized_data = pd.DataFrame({\"id\":data[\"id\"], \"sentiment\":data[\"sentiment\"], \"review\":tokenized_lines})\n",
        "print(tokenized_data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBRP92tJiACW",
        "outputId": "6d28ea10-7261-4935-e8c4-eeff9e3fc915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       id  sentiment                                             review\n",
            "0  5814_8          1  With all this stuff going down at the moment w...\n",
            "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
            "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
            "3  3630_4          0  It must be assumed that those who praised this...\n",
            "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
            "       id  sentiment                                             review\n",
            "0  5814_8          1  [with, all, this, stuff, going, down, at, the,...\n",
            "1  2381_9          1  [the, classic, war, of, the, worlds, by, timot...\n",
            "2  7759_3          0  [the, film, starts, with, a, manager, nicholas...\n",
            "3  3630_4          0  [it, must, be, assumed, that, those, who, prai...\n",
            "4  9495_8          1  [superbly, trashy, and, wondrously, unpretenti...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape)\n",
        "print(tokenized_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVI1oq_-jKIh",
        "outputId": "fc55968d-5d90-46fa-ef54-e3124ec9cb32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 3)\n",
            "(25000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word2Vec\n",
        "To convert text to a vector representation so that we can train a sentiment classifier, we ran word2vec on the tokenized text, representing each word as a vector with 200 features. We combined the data from all three websites to maximize the size of the data to train the word embeddings on. EXPLAIN IN MORE DETAIL HOW WORD2VEC WORKS, SKIP-GRAM, NEGATIVE SAMPLING, COSINE SIMILARITY"
      ],
      "metadata": {
        "id": "JuEoHr8ivyMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIHgDq86RUY7",
        "outputId": "b48d761a-bf9c-4391-fa5a-08e48eab0e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-14 20:45:24--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.146.69\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.146.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  45.1MB/s    in 37s     \n",
            "\n",
            "2021-12-14 20:46:01 (42.3 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "word_vectors = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "metadata": {
        "id": "8a-lu_5yR3Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#import zipfile \n",
        "\n",
        "#!unzip /content/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "#import gzip\n",
        "#f=gzip.open(\"/content/GoogleNews-vectors-negative300.bin.gz\",'rb')\n",
        "#file_content=f.read()\n",
        "#print(file_content)\n",
        "\n",
        "\n",
        "\n",
        "all_tokenized_sentences = pd.concat([imdb_df, yelp_df, amazon_df])\n",
        "all_tokenized_sentences = all_tokenized_sentences.reset_index(drop=True)\n",
        "all_tokenized_sentences = all_tokenized_sentences.reset_index()\n",
        "all_tokenized_sentences = all_tokenized_sentences.rename(columns = {\"index\": \"UID\"})\n",
        "print(all_tokenized_sentences   )\n",
        "\n",
        "model_w2v = gensim.models.Word2Vec(\n",
        "            all_tokenized_sentences[\"Phrases\"],\n",
        "            size=100, # desired no. of features/independent variables\n",
        "            window=5, # context window size\n",
        "            min_count=1, # Ignores all words with total frequency lower than 2.                                  \n",
        "            sg = 1, # 1 for skip-gram model\n",
        "            hs = 0,\n",
        "            negative = 10, # for negative sampling\n",
        "            workers= 32, # no.of cores\n",
        "            seed = 34\n",
        ")\n",
        "model_w2v.train(all_tokenized_sentences[\"Phrases\"], total_examples= len(all_tokenized_sentences[\"Phrases\"]), epochs=20)\n",
        "#print(all_tokenized_sentences['Phrases'][0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k53KGO0c3Y_d",
        "outputId": "19165897-e236-4fd4-aae0-8fd8b673ae2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       UID                                            Phrases  Labels\n",
            "0        0  [very, very, very, slow, moving, aimless, movi...       0\n",
            "1        1  [not, sure, who, more, lost, flat, characters,...       0\n",
            "2        2  [attempting, artiness, with, black, white, cle...       0\n",
            "3        3         [very, little, music, anything, speak, of]       0\n",
            "4        4  [best, scene, movie, when, gerardo, trying, fi...       1\n",
            "...    ...                                                ...     ...\n",
            "2995  2995  [screen, does, get, smudged, easily, because, ...       0\n",
            "2996  2996  [what, piece, of, junk, i, lose, more, calls, ...       0\n",
            "2997  2997                  [item, does, not, match, picture]       0\n",
            "2998  2998  [only, thing, that, disappoint, me, infra, red...       0\n",
            "2999  2999  [you, can, not, answer, calls, with, unit, nev...       0\n",
            "\n",
            "[3000 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(470924, 573820)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_w2v.wv.most_similar(\"dinner\", topn=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1ei_mfb_Bim",
        "outputId": "5918e0a8-5741-442c-c21d-0e17319874a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('lange', 0.9288738369941711), ('become', 0.9264037609100342), ('bachi', 0.9216042757034302), ('hostess', 0.918390154838562), ('marrow', 0.9169269800186157), ('update', 0.9166852235794067), ('putting', 0.914053201675415), ('bed', 0.9140514135360718), ('regular', 0.9127137660980225), ('ignored', 0.9120527505874634)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our model, the data to be classified is a sentence, not individual words. So now that we have trained word2vec and can embed each word as a vector, we take the average of the vectors for all tokens in a sentence to get a vector representation of the entire sentence. "
      ],
      "metadata": {
        "id": "-LBZ9W4JxI-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "count = 0\n",
        "#wordvec_arrays = np.zeros(len(all_tokenized_sentences[\"Phrases\"]), 200)\n",
        "sentence_embeddings = np.zeros((3000, 100))\n",
        "num = 0\n",
        "for phrase in all_tokenized_sentences[\"Phrases\"]:\n",
        "  vec = np.zeros(100).reshape((1, 100))\n",
        "  for word in phrase:\n",
        "    try:\n",
        "        vec += model_w2v[word].reshape((1, 100))\n",
        "        count += 1\n",
        "    except KeyError:\n",
        "      continue\n",
        "    if count != 0:\n",
        "      vec /= count\n",
        "  sentence_embeddings[num]= vec\n",
        "  num += 1\n",
        "  count = 0\n",
        "\n",
        "#m = np.mean(sentence_embeddings)\n",
        "#st = np.std(sentence_embeddings, dtype=np.float64)\n",
        "\n",
        "#for i in sentence_embeddings:\n",
        "  #for j in range(len(i)):\n",
        "   # i[j] = (i[j] - m)/st\n",
        "\n",
        "\n",
        "#print(sentence_embeddings[:2])\n",
        "\n",
        "#sentence_embeddings = sentence_embeddings.transpose(2,0,1).reshape(3,-1)\n",
        "embeddings_df = pd.DataFrame(sentence_embeddings)\n",
        "#print(embeddings_df.isnull().sum().sum())\n",
        "#print((embeddings_df.std(axis=1) == 0).sum().sum())\n",
        "#print(embeddings_df.mean(axis=1).isnull().sum().sum())\n",
        "#print(len(embeddings_df.std(axis=1)))\n",
        "#print(embeddings_df.mean(axis=1)[0])\n",
        "count = 0\n",
        "#for i in range(len(embeddings_df.mean(axis=1).isnull())):\n",
        " # if embeddings_df.std(axis=1)[i] == True:\n",
        "   # print(i)\n",
        "\n",
        "print(embeddings_df.mean().mean())\n",
        "print(embeddings_df.std().std())\n",
        "#embeddings_df1 = embeddings_df.sub(embeddings_df.min().min())\n",
        "#embeddings_df2 = embeddings_df1.div((embeddings_df.max().max() - embeddings_df.min().min()))\n",
        "#embeddings_df = embeddings_df2\n",
        "embeddings_df1 = embeddings_df.sub(embeddings_df.mean(axis=1),axis = 'rows')\n",
        "embeddings_df2 = embeddings_df1.div(embeddings_df.std(axis=1),axis = 'rows')\n",
        "embeddings_df = embeddings_df2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "embeddings_df[\"Labels\"] = all_tokenized_sentences[\"Labels\"]\n",
        "min = 0\n",
        "for i in embeddings_df.min():\n",
        "  if i < min:\n",
        "    min = i\n",
        "print(min)    \n",
        "max = 0\n",
        "for i in embeddings_df.max():\n",
        "  if i > max:\n",
        "    max = i\n",
        "print(max)  \n",
        "#print(embeddings_df2)\n",
        "#print(embeddings_df.isnull().sum().sum())\n",
        "embeddings_df = embeddings_df.dropna(how='any',axis=0) \n",
        "#embeddings_df.drop('Labels',axis=1, inplace=True)\n",
        "print(embeddings_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-yK8qqQA3ZZ",
        "outputId": "b78bdfe8-6027-4444-a887-2453ba1eb542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.008380801579187643\n",
            "0.018725636737784185\n",
            "-3.6054464807495363\n",
            "3.787876349855741\n",
            "             0         1         2  ...        98        99  Labels\n",
            "0    -0.730128 -0.632606 -1.525628  ... -1.862945  0.888169       0\n",
            "1     1.285443  0.039475  0.841172  ... -1.438221  0.726320       0\n",
            "2    -1.282141  0.145933 -1.961896  ... -2.412739  1.485084       0\n",
            "3    -1.035003  0.707328 -0.677356  ... -1.224561  0.653487       0\n",
            "4    -1.709333 -0.248560 -1.861558  ... -1.914453  1.256154       1\n",
            "...        ...       ...       ...  ...       ...       ...     ...\n",
            "2995 -1.401742 -0.548999 -0.516806  ... -1.951500  0.535924       0\n",
            "2996 -1.638751  0.376282 -1.264383  ... -1.258569 -0.270860       0\n",
            "2997 -1.386679  0.205948 -1.421735  ... -1.992234  0.121745       0\n",
            "2998 -1.266277  0.409864 -1.896123  ... -2.386592  1.662854       0\n",
            "2999 -0.882469 -0.401198 -0.305942  ... -1.309577  1.700173       0\n",
            "\n",
            "[2998 rows x 101 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTfSaDR_RzkS",
        "outputId": "fea3f86b-fca2-4ecb-c596-b74f866053d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-3.985183726251882e-06\n",
            "0.00043072569659024243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "v_apple = word_vectors[\"apple\"] \n",
        "v_mango = word_vectors[\"mango\"]\n",
        "print(v_apple.shape)\n",
        "print(v_mango.shape)\n",
        "print(v_mango.reshape(1,300).shape)\n",
        "cosine_similarity([v_mango],[v_apple])\n",
        "word_vectors.wv.most_similar(\"dinner\", topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "UCZNb3HJS6ql",
        "outputId": "e10c782b-9072-4c65-948c-979144e56512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9a291f10de98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mv_apple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"apple\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mv_mango\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mango\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_apple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_mango\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_vectors' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pt_sentence_embeddings = np.zeros((3000, 300))\n",
        "num = 0\n",
        "for phrase in all_tokenized_sentences[\"Phrases\"]:\n",
        "  vec = np.zeros(300).reshape((1, 300))\n",
        "  for word in phrase:\n",
        "    try:\n",
        "        vec += word_vectors[word].reshape((1, 300))\n",
        "        count += 1\n",
        "    except KeyError:\n",
        "      continue\n",
        "    if count != 0:\n",
        "      vec /= count\n",
        "  pt_sentence_embeddings[num]= vec\n",
        "  num += 1\n",
        "  count = 0\n",
        "\n",
        "print(pt_sentence_embeddings[:2])\n",
        "#sentence_embeddings = sentence_embeddings.transpose(2,0,1).reshape(3,-1)\n",
        "pt_embeddings_df = pd.DataFrame(pt_sentence_embeddings)\n",
        "#print(pt_embeddings_df[:2])\n",
        "#print(all_tokenized_sentences[\"Labels\"][:2])\n",
        "#print(pt_embeddings_df.shape)\n",
        "#print(pt_embeddings_df[:1])\n",
        "sum = np.zeros(300).reshape((1,300))\n",
        "\n",
        "pt_embeddings_df1 = pt_embeddings_df.sub(pt_embeddings_df.mean(axis=1),axis = 'rows')\n",
        "pt_embeddings_df2 = pt_embeddings_df1.div(pt_embeddings_df.std(axis=1),axis = 'rows')\n",
        "pt_embeddings_df = pt_embeddings_df2\n",
        "\n",
        "\n",
        "\n",
        "pt_embeddings_df[\"Labels\"] = all_tokenized_sentences[\"Labels\"]\n",
        "\n",
        "\n",
        "#print(embeddings_df2)\n",
        "print(pt_embeddings_df.isnull().sum().sum())\n",
        "pt_embeddings_df = pt_embeddings_df.dropna(how='any',axis=0) \n",
        "#embeddings_df.drop('Labels',axis=1, inplace=True)\n",
        "print(pt_embeddings_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQqdw4UVTiVe",
        "outputId": "f50e2d55-b97f-41a0-e8a0-ba9ca1ca0917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.79852732e-02  1.34874607e-02  2.42976590e-03 -6.91709925e-03\n",
            "   7.12158108e-03 -3.74082028e-03 -1.72039271e-02  1.31641859e-04\n",
            "   1.09635908e-02  7.62919531e-04  3.16937715e-03 -2.06804650e-02\n",
            "  -7.23117613e-03 -1.28787153e-02 -9.71366916e-03  2.25327974e-03\n",
            "  -8.96228498e-03  7.22456788e-03  1.76048754e-03 -7.91555433e-03\n",
            "  -1.01615373e-02  9.26663545e-04  1.91333605e-03 -2.24893464e-02\n",
            "   5.42205690e-03 -1.74922017e-02 -6.83247888e-03  2.36653970e-02\n",
            "   3.20949703e-02 -3.73824448e-03  1.35545921e-02  1.17330842e-02\n",
            "  -5.17623773e-03 -1.57497262e-03 -7.14972374e-03  2.06626339e-02\n",
            "   2.04240735e-02 -1.11550336e-02  5.67605353e-03  2.81117341e-02\n",
            "   2.55269738e-03 -3.05485013e-03  1.78088184e-02 -1.44712809e-03\n",
            "   1.96807916e-02 -1.12643858e-02 -7.18977912e-03  6.97322324e-04\n",
            "  -4.29980348e-03  2.51486602e-03 -1.06795131e-02  1.09748586e-02\n",
            "   4.23014165e-03 -1.54960489e-02  7.52638772e-03 -7.89925451e-03\n",
            "  -5.42811469e-03 -4.21271170e-03  4.23682644e-03 -6.84809244e-04\n",
            "   1.52150918e-02  2.63750249e-02 -3.88661676e-03 -1.66574709e-03\n",
            "   8.01044912e-03 -8.65092401e-04 -2.59984825e-03 -9.18097579e-03\n",
            "   1.13609503e-02 -1.06586472e-03  6.58511977e-03  1.27014991e-02\n",
            "  -2.32934232e-02  3.72108772e-03 -1.04751216e-02  1.20476775e-02\n",
            "  -2.53001118e-03 -4.75584759e-03  5.21538298e-04 -9.67347960e-04\n",
            "   2.61547899e-02 -7.14523805e-03  2.14858358e-03 -1.53800422e-02\n",
            "  -1.08002723e-02  8.50191563e-03 -2.26856201e-02 -2.06623351e-03\n",
            "   1.50921875e-02 -4.36446274e-03 -1.29192529e-02 -1.17216244e-03\n",
            "  -3.86525660e-03 -4.85738312e-03  1.00707878e-02 -1.52510309e-03\n",
            "  -9.76076174e-03  8.12230427e-03  4.47742172e-03 -1.47241355e-02\n",
            "  -1.03863033e-02 -1.21869127e-02  1.58466277e-02 -5.62156700e-03\n",
            "   2.43737081e-02 -1.38030066e-02  1.90792778e-02 -1.53790340e-03\n",
            "   2.79415305e-03 -1.68889235e-02  4.66276344e-03  6.19997466e-03\n",
            "  -2.11202708e-03 -1.41559426e-03  1.61756716e-03 -2.46851766e-03\n",
            "  -6.80693819e-03 -3.07513123e-03  1.30167878e-03  1.71112376e-02\n",
            "   4.43688235e-03 -3.00292546e-03 -2.24118255e-02 -8.40068726e-03\n",
            "  -1.86951557e-02 -7.72037567e-03  4.74368592e-03 -1.78508854e-03\n",
            "   7.66126158e-03  3.42103833e-03 -5.14454837e-03 -2.28426193e-02\n",
            "  -1.17942978e-02 -1.53993035e-04 -1.29661634e-02 -9.45598278e-04\n",
            "  -3.90011109e-03  2.43290344e-03 -7.35987579e-03  2.23447385e-02\n",
            "   2.04545046e-02  5.02726943e-03  1.10229305e-02  1.43618604e-02\n",
            "  -1.12151436e-02  1.70860998e-02 -7.81716607e-03 -2.96307946e-03\n",
            "  -1.06428668e-02 -1.66653140e-03  1.41931451e-02 -1.77799643e-02\n",
            "  -6.07750211e-03 -4.95389956e-03  1.01607771e-03  1.67132358e-02\n",
            "   2.55195593e-03 -5.75639614e-03  1.56112215e-02 -1.41356304e-02\n",
            "  -1.45175600e-02  3.32384005e-03  2.35595003e-02 -7.15669945e-04\n",
            "  -2.57323325e-03 -1.42617116e-02  1.64658813e-02 -6.11090215e-03\n",
            "   7.21257754e-03 -4.00491946e-03  5.89985838e-03  5.11974051e-03\n",
            "  -1.30490755e-02 -1.79344544e-02 -1.08093334e-02 -3.14593564e-03\n",
            "   5.27497177e-03  5.27376139e-03 -1.46046370e-02 -1.27817142e-02\n",
            "   1.20687954e-02  1.32341307e-03  1.93486519e-03 -1.54891434e-02\n",
            "  -1.73357908e-02  6.25917286e-03  7.24386557e-03  1.26465187e-02\n",
            "  -2.44666148e-03 -7.62924609e-03 -1.55882721e-03  1.65428528e-02\n",
            "   1.10263475e-02  1.74781824e-02  1.78160780e-03 -5.69328857e-03\n",
            "  -9.43549048e-03 -4.99217538e-03 -1.41637859e-02 -7.11145060e-03\n",
            "  -7.48474230e-03 -9.30733189e-03 -1.33982537e-02 -8.25768012e-03\n",
            "  -4.85974260e-03 -1.46743358e-02  9.62408383e-03  5.20993361e-03\n",
            "   2.26286090e-03 -2.87679296e-02 -7.61783865e-03 -7.17437750e-03\n",
            "   3.78625993e-03  2.81953376e-03 -2.10288615e-02 -1.15076807e-02\n",
            "  -4.47395649e-03  1.01193152e-02 -2.92099804e-02  1.38782411e-02\n",
            "   4.63256746e-03 -1.49836910e-02  1.75520337e-02 -6.44124641e-03\n",
            "   1.54048287e-02 -1.11513860e-03  2.60385265e-02 -9.99686074e-03\n",
            "  -1.15177506e-02 -5.20001188e-03  5.80035271e-03 -1.88611201e-02\n",
            "  -4.67080412e-03  1.61898779e-02 -6.39645522e-03 -2.73806591e-03\n",
            "  -1.37245185e-02 -8.44665457e-04  2.42294895e-03  3.61380487e-03\n",
            "   1.63015106e-02  1.90641679e-03 -1.70700008e-02 -8.50820717e-03\n",
            "  -1.32612498e-03  9.12086540e-04  1.79308208e-03 -2.26499980e-03\n",
            "   5.19085894e-03  2.69093092e-03  1.18415806e-02  4.34531421e-03\n",
            "  -2.99341434e-03  1.17689770e-02  1.27824267e-02  7.85505503e-03\n",
            "  -4.59508754e-03 -6.24036878e-03 -1.28089788e-02  1.28637950e-03\n",
            "   7.88102009e-03 -1.13369989e-02 -9.45195862e-03 -1.00119684e-02\n",
            "   1.24244258e-02  2.52404278e-02 -4.09092969e-03  1.06303520e-02\n",
            "  -2.06562297e-02 -7.12541017e-03 -1.10379240e-02  1.75357120e-03\n",
            "  -3.30092649e-03 -6.54036736e-03  1.62848593e-03  1.41883849e-02\n",
            "   8.72443091e-03 -1.15852026e-02 -1.43540111e-02 -2.61757141e-03\n",
            "   9.94663905e-03 -9.82742420e-03  5.58815120e-03  3.31488394e-02\n",
            "  -6.28557523e-03 -5.47009441e-03 -1.05918946e-02 -1.31519660e-02\n",
            "   6.22773726e-03  1.20396916e-02  1.33250983e-02 -1.84718701e-03\n",
            "  -1.81790603e-02 -1.28308291e-02 -1.04938375e-02 -4.99409239e-03\n",
            "  -5.81378369e-03 -2.57809061e-02 -5.88606118e-03  2.20730812e-03]\n",
            " [ 9.40012258e-03 -1.18015406e-02  1.87546189e-03  1.42343516e-02\n",
            "  -7.83717209e-03 -1.38114856e-02 -2.58301516e-05 -2.19782601e-02\n",
            "   1.25487199e-03  7.83439714e-03 -7.20218888e-04 -2.18348425e-02\n",
            "   1.28372144e-02  3.96270496e-03 -7.50957676e-03 -4.29714192e-03\n",
            "   8.42865692e-03  4.59236094e-03  2.59977379e-03 -1.01429169e-02\n",
            "   9.48681847e-03  1.34349367e-02 -2.89993695e-03 -3.95145830e-03\n",
            "   8.58644035e-03  3.37621498e-03  2.56609360e-03 -2.47392502e-03\n",
            "   9.28863693e-03 -5.87763464e-03  6.61549925e-03  1.51459501e-02\n",
            "   2.47789968e-03  4.25049842e-03 -1.03230066e-02  4.14519066e-03\n",
            "  -4.17891807e-03 -7.83968816e-03 -1.57749343e-03  1.38951331e-02\n",
            "   1.44861514e-02 -2.64751062e-02  2.96017841e-03 -5.00425348e-03\n",
            "  -1.30114549e-02  4.10066437e-03 -1.58998729e-02 -1.56944338e-03\n",
            "   3.18277908e-03  5.22607605e-04 -1.24982227e-03  1.75345858e-02\n",
            "  -8.22320701e-03 -4.58824704e-03 -2.11346346e-02 -7.21975338e-03\n",
            "  -3.07814454e-03 -3.78229002e-03  1.26162106e-02 -9.40271296e-03\n",
            "   8.69902886e-03 -2.22639047e-03 -3.60568018e-03 -1.08520714e-02\n",
            "  -1.03304722e-02 -8.15953702e-03  9.62529554e-03  1.24816145e-02\n",
            "   7.49873870e-03  9.71291224e-03  4.15900664e-03  5.33357642e-04\n",
            "   1.73008562e-02  1.03042201e-02 -1.19043574e-02  2.64691582e-03\n",
            "   3.84408951e-03  4.82994659e-03 -6.60180929e-03  5.80908110e-03\n",
            "  -9.69622005e-03 -4.39134841e-03  1.28762849e-03 -3.51241814e-03\n",
            "   2.44664967e-03 -3.50917982e-03 -7.11723847e-03  1.87745340e-02\n",
            "   5.70492648e-03 -1.16171541e-03 -4.95885532e-05  1.02374571e-02\n",
            "   5.34534917e-03 -1.50074128e-02 -5.69439898e-03 -7.29704391e-03\n",
            "   1.28177065e-02 -2.38577704e-03 -1.05967579e-02 -5.24142369e-03\n",
            "  -1.46021717e-02 -2.28296527e-02  6.56192826e-03  1.69587626e-02\n",
            "  -1.23186010e-02 -3.67530165e-03 -2.54876585e-03  5.82312806e-03\n",
            "   1.28024494e-02 -1.91524822e-03 -6.04170181e-03  8.03720974e-03\n",
            "  -4.08199896e-03 -1.64720006e-02  4.55440192e-03  5.50682792e-03\n",
            "   6.47867390e-03  1.20062534e-02  5.12484858e-03  1.50979191e-02\n",
            "  -1.52181961e-02  6.80410926e-03 -7.79505477e-03 -7.19454618e-04\n",
            "   1.17187760e-03 -9.93941934e-04 -4.85857641e-03  2.73946679e-03\n",
            "  -5.33957162e-03 -6.56304610e-03 -3.36250026e-03 -1.28222020e-02\n",
            "  -1.37030882e-03 -3.21223177e-03 -7.85715601e-03  8.29484475e-03\n",
            "   5.13444716e-03 -1.12185080e-03 -2.64925832e-03  8.00490327e-03\n",
            "   5.13547508e-03 -2.00554495e-03  3.71974447e-04  3.56953927e-03\n",
            "  -2.70157344e-03  6.87794500e-03 -1.06078156e-02 -8.22959539e-03\n",
            "  -1.35591688e-02  1.47195239e-02  5.10372323e-03  3.78105747e-03\n",
            "  -1.22465611e-02  4.89111593e-03  2.09886951e-03  2.14718071e-02\n",
            "  -7.24198820e-03 -5.65912098e-03  5.16736404e-03  5.48010150e-03\n",
            "  -1.12312217e-03 -9.35214499e-04  7.41805960e-03 -1.47112960e-02\n",
            "   7.64602221e-03 -1.88182786e-02 -1.26821593e-03  6.28620693e-04\n",
            "   2.92030303e-03 -1.48081292e-02 -5.14090477e-03  2.33591595e-04\n",
            "  -9.07095318e-03  8.38513135e-03 -1.16018051e-02  1.83340326e-03\n",
            "   7.29646389e-03 -1.09907238e-02 -2.52919283e-03  4.86644645e-03\n",
            "  -7.83217183e-03 -3.21219891e-03  1.39480949e-02  1.90051452e-02\n",
            "   6.75219596e-03  2.23684816e-03 -8.06350521e-03  1.01083907e-02\n",
            "   3.42569996e-03  6.12263668e-03 -3.66517406e-03  6.16168971e-03\n",
            "  -6.05192707e-03 -7.83973513e-03 -1.31541910e-02 -8.83037720e-04\n",
            "  -2.23049591e-03 -1.02790775e-02 -3.66800608e-03 -9.51062660e-03\n",
            "   1.42995337e-02 -1.14379668e-03 -1.13875477e-02 -4.16683356e-03\n",
            "   1.04129147e-02  1.07905423e-02 -1.69337362e-02 -4.32628739e-03\n",
            "   2.55651416e-04  5.64611413e-03  3.93302609e-03 -4.61151878e-04\n",
            "  -1.24006293e-02  6.26772222e-03  3.73224674e-03  5.29524930e-03\n",
            "   3.96933092e-03 -2.36869831e-03 -1.59430452e-02  5.15367299e-03\n",
            "   3.77948365e-03  6.33595714e-03  3.87450844e-03  3.25831790e-03\n",
            "   5.88248663e-03 -7.95261612e-03  1.29329876e-02  2.39382898e-02\n",
            "  -5.79359397e-03 -1.23605141e-02  5.82829331e-03 -1.33662273e-02\n",
            "   8.21101767e-03 -6.07437396e-03  2.49269320e-03  1.42720193e-02\n",
            "   5.55379105e-03  5.97541335e-03  2.01389420e-02  1.72492306e-02\n",
            "  -6.16424524e-03  1.36438043e-03 -5.49864982e-03 -1.13605173e-02\n",
            "  -7.76741763e-03 -1.19303191e-03  3.56734500e-03  8.45649801e-03\n",
            "   1.28406629e-03  3.52800524e-03 -4.62100105e-03 -8.92075677e-03\n",
            "   1.46307359e-03 -8.17982396e-04  1.63760494e-03  1.37079139e-03\n",
            "  -1.82807461e-04 -4.33570640e-03 -2.85137198e-03 -6.44202236e-03\n",
            "  -1.23344471e-02  3.37905466e-03 -1.78734678e-02 -7.83184633e-04\n",
            "  -1.22404485e-03  9.45138811e-03  3.19991050e-03  1.04209606e-02\n",
            "  -2.38224980e-04  9.86840016e-04  5.66476749e-03 -9.72566291e-03\n",
            "   1.10111060e-03  7.92568914e-03  1.75794245e-02 -8.02779957e-03\n",
            "  -1.43139971e-03 -1.82452234e-02  1.61771281e-03 -2.71363161e-03\n",
            "   8.65836459e-03 -5.52500105e-03 -8.24892365e-03  9.80611600e-03\n",
            "   7.03369164e-03  1.16050187e-02 -1.64396685e-02  1.12652481e-03\n",
            "   7.92304021e-03 -1.72976387e-02 -1.89014483e-03 -2.01403560e-03\n",
            "  -1.73190691e-02 -3.31837986e-03 -1.48420180e-02  1.07215052e-02\n",
            "   1.91471976e-03  9.18938629e-03 -1.25187468e-02  1.32050446e-03]]\n",
            "600\n",
            "             0         1         2  ...       298       299  Labels\n",
            "0     2.491268  1.229673  0.267436  ... -0.456205  0.248078       0\n",
            "1     1.070127 -1.282909  0.235013  ... -1.362507  0.173422       0\n",
            "2    -0.532916  0.780772  0.533502  ... -1.148226  0.691602       0\n",
            "3     0.127483 -0.783035  0.478418  ...  1.332333 -0.820429       0\n",
            "4    -0.388840 -0.496270  1.532459  ...  0.125177  0.852132       1\n",
            "...        ...       ...       ...  ...       ...       ...     ...\n",
            "2995  1.121790  0.849876  1.817344  ...  1.095097 -0.352224       0\n",
            "2996  0.038349 -0.621548 -0.691848  ...  0.131260 -1.054985       0\n",
            "2997  0.492930  1.430637 -0.371558  ... -0.261535 -1.338744       0\n",
            "2998  0.537123  0.944399  0.045745  ...  0.162120  1.034762       0\n",
            "2999 -0.224047  0.537295  0.512398  ...  1.394704 -1.195960       0\n",
            "\n",
            "[2998 rows x 301 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bigger dataset\n",
        "\n",
        "big_embeddings = np.zeros((25000, 300))\n",
        "num = 0\n",
        "for phrase in data[\"review\"]:\n",
        "  vec = np.zeros(300).reshape((1, 300))\n",
        "  for word in phrase:\n",
        "    try:\n",
        "        vec += word_vectors[word].reshape((1, 300))\n",
        "        count += 1\n",
        "    except KeyError:\n",
        "      continue\n",
        "    if count != 0:\n",
        "      vec /= count\n",
        "  big_embeddings[num]= vec\n",
        "  num += 1\n",
        "  count = 0\n",
        "\n",
        "big_embeddings_df = pd.DataFrame(big_embeddings)\n",
        "big_embeddings_df1 = big_embeddings_df.sub(big_embeddings_df.mean(axis=1),axis = 'rows')\n",
        "big_embeddings_df2 = big_embeddings_df1.div(big_embeddings_df.std(axis=1),axis = 'rows')\n",
        "big_embeddings_df = big_embeddings_df2\n",
        "\n",
        "big_embeddings_df[\"Labels\"] = data['sentiment']\n",
        "\n",
        "\n",
        "#print(embeddings_df2)\n",
        "print(big_embeddings_df.isnull().sum().sum())\n",
        "big_embeddings_df = big_embeddings_df.dropna(how='any',axis=0) \n",
        "#embeddings_df.drop('Labels',axis=1, inplace=True)\n",
        "print(big_embeddings_df)  \n",
        "\"\"\"\n",
        "print(big_embeddings[:2])\n",
        "#sentence_embeddings = sentence_embeddings.transpose(2,0,1).reshape(3,-1)\n",
        "\n",
        "print(big_embeddings_df[:2])\n",
        "print(big_embeddings_df.shape)\n",
        "print(big_embeddings_df[:1])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "5OOCeptRjUHE",
        "outputId": "8b7ca958-7139-4cad-fd99-fe5f553da6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "              0         1         2  ...       298       299  Labels\n",
            "0     -2.212821  1.450381 -0.226155  ... -0.866486  2.194698       1\n",
            "1     -2.439197  1.342694  0.309873  ...  0.850287  2.751547       1\n",
            "2     -0.159698  1.826075 -0.425596  ... -1.454917  0.942753       0\n",
            "3     -0.980355  1.041724 -0.314449  ... -0.221568 -1.120664       0\n",
            "4     -0.287194  0.973776 -0.035684  ... -0.984885  1.643056       1\n",
            "...         ...       ...       ...  ...       ...       ...     ...\n",
            "24995 -2.438734  1.343486  0.311355  ...  0.850847  2.751584       0\n",
            "24996 -1.977057  1.216293 -1.733094  ...  0.237154  0.582399       0\n",
            "24997  0.830364 -0.661113  0.556532  ... -0.313325  1.510859       0\n",
            "24998 -0.287257  0.975322 -0.035872  ... -0.985168  1.643689       0\n",
            "24999 -0.527713  1.411506  0.478683  ... -0.851087  0.268782       1\n",
            "\n",
            "[25000 rows x 301 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(big_embeddings[:2])\\n#sentence_embeddings = sentence_embeddings.transpose(2,0,1).reshape(3,-1)\\n\\nprint(big_embeddings_df[:2])\\nprint(big_embeddings_df.shape)\\nprint(big_embeddings_df[:1])\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVM Model\n",
        "Our first approach was to use an svm model for binary classification. The two classes are 1 for positive sentiment, and 0 for negative sentiment. We trained the model on the vector representations of each sentence. "
      ],
      "metadata": {
        "id": "RjkB_Gzexwyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was in this step that we found our first major hurdle. When we looked at our model's f1 score and accuracy metrics, we seemed to get the same exact numbers regardless of what parameters we modified. These scores were also very low: in our initial attempt our accuracy was sub 50%. The confusion matrix for this model indicated that every single prediction was either a true positive or false positive, so there was clearly something fundamentally wrong with our model. Our guess was that the word embeddings trained on the dataset were not useful due to the small size of the dataset. Even when combining all 3 example datasets, we only had a total of 3000 example sentences, and many of the sentences were only 4 words or less. To get useful embeddings of these words, that are intended to encode meaning and context, we would need a much larger dataset. To remedy this, we decided to use a pretrained version of word2vec that was trained on an enormous quantity of sentences in our preprocessing, and retrain our model on these new sentence embeddings. "
      ],
      "metadata": {
        "id": "HNS7liDAyBtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix \n",
        "\n",
        "#train_w2v = embeddings_df.iloc[:2400,:]\n",
        "#test_w2v = embeddings_df.iloc[2400:,:]\n",
        "\n",
        "embeddings_df_no_labels  = embeddings_df.loc[:,embeddings_df.columns != 'Labels']\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(embeddings_df_no_labels , embeddings_df['Labels'], random_state=42, test_size=0.25)\n",
        "print(xtrain[:10])\n",
        "print(ytrain[:10])\n",
        "\n",
        "svc = svm.SVC(kernel='linear', C=1, gamma = 1, probability=True).fit(xtrain, ytrain) \n",
        "prediction = svc.predict_proba(xtest) \n",
        "prediction_int = prediction[:,1] >= 0.5\n",
        "prediction_int = prediction_int.astype(np.int) \n",
        "#prediction = svc.predict(xtrain) \n",
        "print(f1_score(ytest, prediction_int))\n",
        "print(accuracy_score(ytest, prediction_int))\n",
        "print(confusion_matrix(ytest, prediction_int))\n",
        "TN, FP, FN, TP = confusion_matrix(ytest, prediction_int).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHFVwzDNExdp",
        "outputId": "231ab20e-f29e-45e9-9679-f1cde5457504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            0         1         2   ...        97        98        99\n",
            "1066 -1.134841 -0.269029 -1.297176  ...  0.394902 -1.825627  1.860990\n",
            "663  -1.511112  0.227453 -1.663059  ... -0.089047 -2.112329  2.020554\n",
            "481  -1.314494  0.242423 -1.921818  ...  0.482243 -2.306766  1.840990\n",
            "2138 -1.405848  0.369150 -2.150020  ...  0.671640 -2.063225  1.369380\n",
            "2703 -1.384383  0.619844 -1.763493  ...  0.977955 -2.440726  1.279768\n",
            "1338 -0.481294  0.508624 -0.697093  ...  0.896794 -1.591527  1.697029\n",
            "2981 -1.339690  1.803891 -1.933986  ...  0.186324 -1.978891  1.125692\n",
            "2048 -1.577000  1.192710 -2.178420  ...  0.151728 -1.690598  1.745137\n",
            "1243 -1.782035 -0.174848 -2.155849  ...  0.342310 -2.173884  0.739187\n",
            "2708 -1.503534  0.369664 -1.836620  ...  0.367803 -1.394882  1.766591\n",
            "\n",
            "[10 rows x 100 columns]\n",
            "1066    1\n",
            "663     1\n",
            "481     0\n",
            "2138    0\n",
            "2703    1\n",
            "1338    1\n",
            "2981    0\n",
            "2048    1\n",
            "1243    0\n",
            "2708    0\n",
            "Name: Labels, dtype: int64\n",
            "0.6763540290620872\n",
            "0.6733333333333333\n",
            "[[249 120]\n",
            " [125 256]]\n",
            "TP = 256\n",
            "FP = 120\n",
            "FN = 125\n",
            "TN = 249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pt_embeddings_df_no_labels  = pt_embeddings_df.loc[:,pt_embeddings_df.columns != 'Labels']\n",
        "xtrain_pt, xtest_pt, ytrain_pt, ytest_pt = train_test_split(pt_embeddings_df_no_labels, pt_embeddings_df['Labels'], random_state=42, test_size=0.25)\n",
        "print(xtrain_pt[:10])\n",
        "print(ytrain_pt[:10])\n",
        "\n",
        "svc_pt = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_pt, ytrain_pt) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqnP9SfQT-pB",
        "outputId": "c49cd47d-70bb-4d8a-b7aa-c75539aeb257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           0         1         2    ...       297       298       299\n",
            "1761  0.848872  0.672286 -0.698902  ... -0.366251 -1.542368  0.880810\n",
            "2795 -1.118913 -0.251022 -0.213904  ... -0.302753 -0.988999  0.999010\n",
            "659   0.097256 -0.336943  0.372916  ... -1.496920 -0.318650 -0.543134\n",
            "1194  0.663193 -0.296420 -0.446368  ... -0.875605 -1.009303  0.219543\n",
            "1042  1.156703  0.905345 -0.363905  ...  0.953512  0.507924 -0.107910\n",
            "2623  1.149816 -0.014923  0.411930  ...  0.406199 -0.198657 -0.123025\n",
            "2587 -0.484713  1.161837  1.076292  ...  0.979939  0.333367  0.193056\n",
            "2107 -1.012698  1.330769 -1.029547  ...  0.361821  1.017335 -0.356732\n",
            "2433 -0.108252 -0.779854 -0.693216  ... -0.152409  0.291147 -0.689624\n",
            "2257  0.530908  1.535280 -0.207971  ... -1.236642 -0.152173 -1.049447\n",
            "\n",
            "[10 rows x 300 columns]\n",
            "1761    0\n",
            "2795    1\n",
            "659     1\n",
            "1194    0\n",
            "1042    0\n",
            "2623    0\n",
            "2587    0\n",
            "2107    1\n",
            "2433    1\n",
            "2257    1\n",
            "Name: Labels, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_pt = svc_pt.predict_proba(xtest_pt) \n",
        "prediction_int_pt = prediction_pt[:,1] >= 0.5\n",
        "prediction_int_pt = prediction_int_pt.astype(np.int) \n",
        "print(f1_score(ytest_pt, prediction_int_pt))\n",
        "print(accuracy_score(ytest_pt, prediction_int_pt))\n",
        "print(confusion_matrix(ytest_pt, prediction_int_pt))\n",
        "TN, FP, FN, TP = confusion_matrix(ytest_pt, prediction_int_pt).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svR9X9_suxPn",
        "outputId": "206c5286-3fc6-4654-fb14-d2faba2b1c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6344086021505376\n",
            "0.6358768406961178\n",
            "[[239 133]\n",
            " [139 236]]\n",
            "TP = 236\n",
            "FP = 133\n",
            "FN = 139\n",
            "TN = 239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(xtest.loc[0])\n",
        "print(svc)\n",
        "prediction = svc.predict_proba([xtest.loc[7]])\n",
        "print(prediction)\n",
        "prediction_int = prediction[:,1] >= 0.5\n",
        "print(prediction_int)\n",
        "prediction_int = prediction_int.astype(np.int) \n",
        "print(prediction_int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjSMxhIOJdnE",
        "outputId": "c24926a7-76dc-4f6c-e8fe-d975c65b9c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC(C=1, gamma=1, kernel='linear', probability=True)\n",
            "[[0.62835496 0.37164504]]\n",
            "[False]\n",
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Big dataset svm\n",
        "big_embeddings_df_no_labels  = big_embeddings_df.loc[:,big_embeddings_df.columns != 'Labels']\n",
        "xtrain_big, xtest_big, ytrain_big, ytest_big = train_test_split(big_embeddings_df_no_labels, big_embeddings_df['Labels'], random_state=42, test_size=0.25)\n",
        "print(xtrain_big[:10])\n",
        "print(ytrain_big[:10])\n",
        "\n",
        "svc_big = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_big, ytrain_big) \n",
        "prediction_big = svc_big.predict_proba(xtest_big) \n",
        "prediction_int_big = prediction_big[:,1] >= 0.5\n",
        "prediction_int_big = prediction_int_big.astype(np.int) \n",
        "print(f1_score(ytest_big, prediction_int_big))\n",
        "print(accuracy_score(ytest_big, prediction_int_big))\n",
        "print(confusion_matrix(ytest_big, prediction_int_big))\n",
        "TN, FP, FN, TP = confusion_matrix(ytest_big, prediction_int_big).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPSKKb6Plnt1",
        "outputId": "b70a7aee-cf12-4344-8aad-a78dd6c8bc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            0         1         2    ...       297       298       299\n",
            "6920  -1.980397  1.218343 -1.731330  ... -0.912561  0.240662  0.587081\n",
            "17926 -0.752291  0.355936  0.919886  ... -0.513939 -0.409460  1.195257\n",
            "1123  -2.436951  1.343560  0.309208  ... -0.278999  0.851972  2.750618\n",
            "4518  -0.289672  0.975285 -0.035760  ...  0.447473 -0.984770  1.645191\n",
            "5576  -0.289698  0.973217 -0.034247  ...  0.448660 -0.985622  1.642261\n",
            "14265 -1.978273  1.217246 -1.732899  ... -0.912768  0.239628  0.583597\n",
            "8845  -0.752541  0.357346  0.919970  ... -0.513086 -0.409592  1.197406\n",
            "2223  -0.527726  1.411375  0.478760  ... -0.773530 -0.850963  0.268689\n",
            "23631 -0.161325  1.824720 -0.424586  ... -0.415728 -1.455227  0.941276\n",
            "7315   0.830291 -0.660750  0.556271  ...  0.073973 -0.313184  1.511314\n",
            "\n",
            "[10 rows x 300 columns]\n",
            "6920     1\n",
            "17926    1\n",
            "1123     1\n",
            "4518     1\n",
            "5576     0\n",
            "14265    0\n",
            "8845     1\n",
            "2223     1\n",
            "23631    0\n",
            "7315     0\n",
            "Name: Labels, dtype: int64\n",
            "0.4898921832884098\n",
            "0.51552\n",
            "[[1768 1324]\n",
            " [1704 1454]]\n",
            "TP = 1454\n",
            "FP = 1324\n",
            "FN = 1704\n",
            "TN = 1768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_pt2 = svc_pt.predict(xtest_pt) \n",
        "print(f1_score(ytest_pt, prediction_pt2))\n",
        "print(accuracy_score(ytest_pt, prediction_pt2))\n",
        "print(confusion_matrix(ytest_pt, prediction_pt2))\n",
        "TN, FP, FN, TP = confusion_matrix(ytest_pt, prediction_pt2).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWqGgg8_U5Pe",
        "outputId": "56e46351-384d-43e7-db00-3cb4435a17e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6654804270462633\n",
            "0.49866666666666665\n",
            "[[  0 376]\n",
            " [  0 374]]\n",
            "TP = 374\n",
            "FP = 376\n",
            "FN = 0\n",
            "TN = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Search"
      ],
      "metadata": {
        "id": "kv1AJkYSzU85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf','linear']}\n",
        "\n",
        "grid = GridSearchCV(svm.SVC(probability=True), param_grid, refit = True, verbose = 3)\n",
        " \n",
        "# fitting the model for grid search\n",
        "grid.fit(xtrain, ytrain)\n",
        "\n",
        "print(grid.best_params_)\n",
        " \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid.best_estimator_)"
      ],
      "metadata": {
        "id": "KQ2llezxI27S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1670230-d50d-4b0e-8845-475d88a5de06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "[CV 1/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.502 total time=   1.8s\n",
            "[CV 2/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 1/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 4/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 1/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 3/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 5/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 3/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 4/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 1/5] END C=0.1, gamma=0.0001, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END C=0.1, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END C=0.1, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END C=0.1, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 5/5] END C=0.1, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 4/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 4/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END ..C=1, gamma=0.0001, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END ..C=1, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 3/5] END ..C=1, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 4/5] END ..C=1, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END ..C=1, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END ......C=10, gamma=1, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END ......C=10, gamma=1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 3/5] END ......C=10, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END ......C=10, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END ......C=10, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 3/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 5/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END .C=10, gamma=0.0001, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END .C=10, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END .C=10, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END .C=10, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END .C=10, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END .....C=100, gamma=1, kernel=linear;, score=0.502 total time=   1.1s\n",
            "[CV 2/5] END .....C=100, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END .....C=100, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END .....C=100, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END .....C=100, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.502 total time=   1.1s\n",
            "[CV 2/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 3/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 4/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 5/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END .C=100, gamma=0.001, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END .C=100, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END .C=100, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END .C=100, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END .C=100, gamma=0.001, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 1/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END C=100, gamma=0.0001, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END C=100, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 3/5] END C=100, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END C=100, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 5/5] END C=100, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 1/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 5/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END ....C=1000, gamma=1, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END ....C=1000, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END ....C=1000, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END ....C=1000, gamma=1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END ....C=1000, gamma=1, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 1/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 3/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 5/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.500 total time=   1.8s\n",
            "[CV 1/5] END ..C=1000, gamma=0.1, kernel=linear;, score=0.502 total time=   1.1s\n",
            "[CV 2/5] END ..C=1000, gamma=0.1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 3/5] END ..C=1000, gamma=0.1, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END ..C=1000, gamma=0.1, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 5/5] END ..C=1000, gamma=0.1, kernel=linear;, score=0.502 total time=   1.1s\n",
            "[CV 1/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END .C=1000, gamma=0.01, kernel=linear;, score=0.502 total time=   1.1s\n",
            "[CV 2/5] END .C=1000, gamma=0.01, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 3/5] END .C=1000, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END .C=1000, gamma=0.01, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END .C=1000, gamma=0.01, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 1/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END C=1000, gamma=0.001, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END C=1000, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END C=1000, gamma=0.001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END C=1000, gamma=0.001, kernel=linear;, score=0.500 total time=   1.1s\n",
            "[CV 5/5] END C=1000, gamma=0.001, kernel=linear;, score=0.502 total time=   1.1s\n",
            "[CV 1/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.502 total time=   1.7s\n",
            "[CV 2/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 3/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 4/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 5/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.500 total time=   1.7s\n",
            "[CV 1/5] END C=1000, gamma=0.0001, kernel=linear;, score=0.502 total time=   1.0s\n",
            "[CV 2/5] END C=1000, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 3/5] END C=1000, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 4/5] END C=1000, gamma=0.0001, kernel=linear;, score=0.500 total time=   1.0s\n",
            "[CV 5/5] END C=1000, gamma=0.0001, kernel=linear;, score=0.502 total time=   1.1s\n",
            "{'C': 1000, 'gamma': 1, 'kernel': 'linear'}\n",
            "SVC(C=1000, gamma=1, kernel='linear', probability=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_predictions = grid.predict_proba(xtest)\n",
        "grid_prediction_int = grid_predictions[:,1] >= 0.3\n",
        "grid_prediction_int = grid_prediction_int.astype(np.int) \n",
        "print(f1_score(ytest, grid_prediction_int))\n",
        "print(accuracy_score(ytest, grid_prediction_int))\n",
        "\n",
        "svcbest = svm.SVC(kernel='rbf', C=1000, gamma= 1, probability=True).fit(xtrain, ytrain) \n",
        "prediction2 = svcbest.predict_proba(xtest) \n",
        "prediction_int2 = prediction2[:,1] >= 0.3\n",
        "prediction_int2 = prediction_int2.astype(np.int) \n",
        "print(f1_score(ytest, prediction_int2))\n",
        "print(accuracy_score(ytest, prediction_int2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O939RJq4KVhC",
        "outputId": "e1d070d1-dc52-4b74-9394-957fd02504bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6654804270462633\n",
            "0.49866666666666665\n",
            "0.6654804270462633\n",
            "0.49866666666666665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "1XNz4Bn5b88h",
        "outputId": "c6241edb-74d4-4575-e408-dc5edd91b915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a2be3ff8d54b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GoogleNews-vectors-negative300.bin.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 raise EOFError(\"Compressed file ended before the \"\n\u001b[0m\u001b[1;32m    494\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning approach-first approach equally flawed because of same problem with size of training data for word2vec"
      ],
      "metadata": {
        "id": "uYV_x5TbzZBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                 hidden_layer_sizes=(15, 3), random_state=1,max_iter=10000)\n",
        "clf.fit(xtrain, ytrain)\n",
        "pred = clf.predict(xtest)\n",
        "print(f1_score(ytest, pred))\n",
        "print(accuracy_score(ytest,pred))\n",
        "TN, FP, FN, TP = confusion_matrix(ytest, pred).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))"
      ],
      "metadata": {
        "id": "GCbOXJ_VZ2_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54cc6caf-8c52-4ccd-fac3-74d8305ef212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7020725388601037\n",
            "0.6933333333333334\n",
            "TP = 271\n",
            "FP = 120\n",
            "FN = 110\n",
            "TN = 249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_pt = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                 hidden_layer_sizes=(5, 2), random_state=1,max_iter=10000)\n",
        "clf_pt.fit(xtrain_pt, ytrain_pt)\n",
        "pred_ptc = clf_pt.predict(xtest_pt)\n",
        "print(f1_score(ytest_pt, pred_ptc))\n",
        "print(accuracy_score(ytest_pt,pred_ptc))\n",
        "TN, FP, FN, TP = confusion_matrix(ytest_pt, pred_ptc).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_foMXa_-dv8v",
        "outputId": "fcc46df3-bd49-41e9-9592-f0d9415a0cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7032967032967034\n",
            "0.6746987951807228\n",
            "TP = 288\n",
            "FP = 156\n",
            "FN = 87\n",
            "TN = 216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMPARING VADER RESULTS TO BINARY LABELS\n",
        "VADER is a rules based sentiment algorithm. It can be helpful for establishing a baseline for sentiment. The most common use for VADER where it has been found to have significant success is with tweets. With this specific data, VADER had a 65-70% base accuracy and could be improved to around 80%. For the code exploring this, see the attached FinalProjectVADER jupyter notebook.\n"
      ],
      "metadata": {
        "id": "IAbbwyQ_1cLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GeG-hM0q1Q7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RmG46qGE4TVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zz-9uLgi1j8y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}