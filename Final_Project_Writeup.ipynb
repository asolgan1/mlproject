{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project Writeup.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Presentation: Sentiment Prediction\n",
        "Matthew Landry, Alexey Solganik, Michael Klisiwecz"
      ],
      "metadata": {
        "id": "aYgfWXbzzgMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing\n",
        "Our project is sentiment prediction on sentiment labelled reviews from yelp, imdb, and amazon. Before we can create a model to predict sentiment, we need to preprocess the sentences from the example data set so they can be represented as vectors. First, we remove unnecessary symbols from the sentences (punctuation, numbers, capital letters, extra spaces) and then tokenize each sentence. Initially, we tried to remove stop words as part of our preprocessing step, but it turned out that this would remove words like \"not\", which change the sentiment of a sentence, so we realized we could not include this step even if it means leaving in many words that do not affect sentence sentiment (ex. a, the, is). We used PorterStemmer from the gensim library to remove word stems, because we want as many instances as possible of a given word in our training data, so it is best to consider all forms of a word (ex. large, larger, largest) as the same word. \n",
        "Note: stemming is commented out because it is only used for models in which we use word embeddings trained on the sample data, and causes worse performance with the pre-trained embeddings"
      ],
      "metadata": {
        "id": "R5Mrm4VGttRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Review Dataset (imdb, yelp, amazon)"
      ],
      "metadata": {
        "id": "5u1gnbp06svS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Sn87uqZnpvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8e8ea1-1ed9-4d25-d8a3-1409efb10e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Phrases  Labels\n",
            "0  [very, very, very, slow, moving, aimless, movi...       0\n",
            "1  [not, sure, who, more, lost, flat, characters,...       0\n",
            "2  [attempting, artiness, with, black, white, cle...       0\n",
            "3         [very, little, music, anything, speak, of]       0\n",
            "4  [best, scene, movie, when, gerardo, trying, fi...       1\n",
            "5  [rest, of, movie, lacks, art, charm, meaning, ...       0\n",
            "6                               [wasted, two, hours]       0\n",
            "7  [saw, movie, today, thought, it, good, effort,...       1\n",
            "8                                 [bit, predictable]       0\n",
            "9  [loved, casting, of, jimmy, buffet, as, scienc...       1\n",
            "                                             Phrases  Labels\n",
            "0  [so, there, no, way, for, me, plug, it, here, ...       0\n",
            "1                     [good, case, excellent, value]       1\n",
            "2                              [great, for, jawbone]       1\n",
            "3  [tied, charger, for, conversations, lasting, m...       0\n",
            "4                                       [mic, great]       1\n",
            "5  [i, have, jiggle, plug, get, it, line, up, rig...       0\n",
            "6  [if, you, have, several, dozen, several, hundr...       0\n",
            "7      [if, you, razr, owner, you, must, have, this]       1\n",
            "8              [needless, say, i, wasted, my, money]       0\n",
            "9                     [what, waste, of, money, time]       0\n",
            "                                             Phrases  Labels\n",
            "0                          [wow, loved, this, place]       1\n",
            "1                                 [crust, not, good]       0\n",
            "2                 [not, tasty, texture, just, nasty]       0\n",
            "3  [stopped, by, during, late, may, bank, holiday...       1\n",
            "4               [selection, menu, great, so, prices]       1\n",
            "5  [now, i, am, getting, angry, i, want, my, damn...       0\n",
            "6        [honeslty, it, didn, t, taste, that, fresh]       0\n",
            "7  [potatoes, like, rubber, you, could, tell, the...       0\n",
            "8                                [fries, great, too]       1\n",
            "9                                     [great, touch]       1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    500\n",
              "0    500\n",
              "Name: Labels, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import gensim\n",
        "import pandas as pd\n",
        "from gensim.parsing.preprocessing import strip_numeric, strip_punctuation, strip_multiple_whitespaces\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "pstem = PorterStemmer()\n",
        "\n",
        "filter = ['a', 'an', 'the', 'is', 'are', 'were', 'was', 'will', 'be', 'in', 'to', 'on','at','and','or']\n",
        "\n",
        "with open(\"imdb_labelled.txt\") as dat:\n",
        "    imdb_data = []\n",
        "    imdb_results = []\n",
        "    for line in dat:\n",
        "        imdb_results.append(int(line[len(line)-2:len(line)-1]))\n",
        "        line = line.lower()\n",
        "        line = strip_punctuation(line)\n",
        "        line = strip_multiple_whitespaces(line)\n",
        "        line = strip_numeric(line)\n",
        "\n",
        "        tokenized_line = line.split()\n",
        "        tokenized_line1 = []\n",
        "        for token in tokenized_line:\n",
        "          if token not in filter:\n",
        "            tokenized_line1.append(token)\n",
        "        tokenized_line = tokenized_line1  \n",
        "       # stem_line = lambda x: [pstem.stem(token) for token in x]\n",
        "        #tokenized_line = stem_line(tokenized_line)\n",
        "\n",
        "        imdb_data.append(tokenized_line)\n",
        "        \n",
        "with open(\"amazon_cells_labelled.txt\") as dat:\n",
        "    amazon_data = []\n",
        "    amazon_results = []\n",
        "    for line in dat:\n",
        "        amazon_results.append(int(line[len(line)-2:len(line)-1]))\n",
        "        line = line.lower()\n",
        "        line = strip_punctuation(line)\n",
        "        line = strip_multiple_whitespaces(line)\n",
        "        line = strip_numeric(line)\n",
        "\n",
        "        tokenized_line = line.split()\n",
        "        tokenized_line1 = []\n",
        "        for token in tokenized_line:\n",
        "          if token not in filter:\n",
        "            tokenized_line1.append(token)\n",
        "        tokenized_line = tokenized_line1 \n",
        "        #stem_line = lambda x: [pstem.stem(token) for token in x]\n",
        "        #tokenized_line = stem_line(tokenized_line)\n",
        "\n",
        "        amazon_data.append(tokenized_line)\n",
        "        \n",
        "with open(\"yelp_labelled.txt\") as dat:\n",
        "    yelp_data = []\n",
        "    yelp_results = []\n",
        "    for line in dat:\n",
        "        yelp_results.append(int(line[len(line)-2:len(line)-1]))\n",
        "        line = line.lower()\n",
        "        line = strip_punctuation(line)\n",
        "        line = strip_multiple_whitespaces(line)\n",
        "        line = strip_numeric(line)\n",
        "\n",
        "        tokenized_line = line.split()\n",
        "        tokenized_line1 = []\n",
        "        for token in tokenized_line:\n",
        "          if token not in filter:\n",
        "            tokenized_line1.append(token)\n",
        "        tokenized_line = tokenized_line1 \n",
        "        #stem_line = lambda x: [pstem.stem(token) for token in x]\n",
        "        #tokenized_line = stem_line(tokenized_line)\n",
        "        \n",
        "        yelp_data.append(tokenized_line)\n",
        "        \n",
        "\n",
        "\n",
        "imdb_df = pd.DataFrame({\"Phrases\" : imdb_data, \"Labels\" : imdb_results})\n",
        "amazon_df = pd.DataFrame({\"Phrases\" : amazon_data, \"Labels\" : amazon_results})\n",
        "yelp_df = pd.DataFrame({\"Phrases\" : yelp_data, \"Labels\" : yelp_results})\n",
        "\n",
        "print(imdb_df[:10])\n",
        "print(amazon_df[:10])\n",
        "print(yelp_df[:10])\n",
        "imdb_df[\"Labels\"].value_counts()\n",
        "amazon_df[\"Labels\"].value_counts()\n",
        "yelp_df[\"Labels\"].value_counts()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above output shows examples of tokenized sentences from our example dataset of user reviews from imdb, amazon, and yelp. Each sentence has been tokenized and stripped of unneccessary symbols. "
      ],
      "metadata": {
        "id": "WpQuMYDNvUSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##IMDB Big Dataset\n",
        "Below is the preprocessing for a larger dataset of labeled imdb reviews (25000 samples). This section does not contain stemming/filtering because we did not have a chance to do as much as we had planned with this dataset because using it was not one of the primary goals of our project. "
      ],
      "metadata": {
        "id": "YrNWjuEf6zVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"IMDB_data.txt\", sep='\\t')"
      ],
      "metadata": {
        "id": "oRjW7ERDhXjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_lines = []\n",
        "for line in data[\"review\"]:\n",
        "  line = line.lower()\n",
        "  line = strip_punctuation(line)\n",
        "  line = strip_multiple_whitespaces(line)\n",
        "  line = strip_numeric(line)\n",
        "\n",
        "  tokenized_lines.append(line.split())\n",
        "\n",
        "tokenized_data = pd.DataFrame({\"id\":data[\"id\"], \"sentiment\":data[\"sentiment\"], \"review\":tokenized_lines})"
      ],
      "metadata": {
        "id": "nBRP92tJiACW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape)\n",
        "print(tokenized_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVI1oq_-jKIh",
        "outputId": "fc55968d-5d90-46fa-ef54-e3124ec9cb32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 3)\n",
            "(25000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec\n",
        "To convert text to a vector representation so that we can train a binary classifier for sentiment prediction, we ran the gensim library's word2vec model on the tokenized text, representing each word as a vector with 100 features. We combined the data from all three websites to maximize the size of the data to train the word embeddings on, as we only have 1000 samples from each website. We used the skip-gram model version of word2vec, which predicts probability of context given a word. \n",
        "\n",
        "We also used google's pretrained word2vec model, which has a vocabulary of around 3 million different words, and has 300 features for each, so is far more comprehensive than our own trained word2vec model. We wanted to see which would provide better word embeddings for our classification models. While google's word2vec is far larger, our own word2vec is trained on the same contexts those words appear in the training data, so it was unclear which would have better performance. "
      ],
      "metadata": {
        "id": "JuEoHr8ivyMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIHgDq86RUY7",
        "outputId": "95bd046a-7833-4dee-f75b-61688e235cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-15 02:53:27--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.248.54\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.248.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "word_vectors = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "metadata": {
        "id": "8a-lu_5yR3Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokenized_sentences = pd.concat([imdb_df, yelp_df, amazon_df])\n",
        "all_tokenized_sentences = all_tokenized_sentences.reset_index(drop=True)\n",
        "all_tokenized_sentences = all_tokenized_sentences.reset_index()\n",
        "all_tokenized_sentences = all_tokenized_sentences.rename(columns = {\"index\": \"UID\"})\n",
        "\n",
        "#sg specifies skip-gram variant, hs and negative specify negative sampling\n",
        "model_w2v = gensim.models.Word2Vec(all_tokenized_sentences[\"Phrases\"],size=100, window=5, min_count=1, sg = 1, hs = 0, negative = 10)\n",
        "#we used a high number of epochs (iterations over corpus) because of the small number of words we are training on\n",
        "model_w2v.train(all_tokenized_sentences[\"Phrases\"], total_examples= len(all_tokenized_sentences[\"Phrases\"]), epochs=20)\n"
      ],
      "metadata": {
        "id": "k53KGO0c3Y_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef25e66-2e36-469e-e353-86b1e7584afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(470488, 573820)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using word2vec, the cosine similarity between the vectors corresponding to the word embeddings of 2 words approximates how close the words are in semantic meaning. Most similar outputs the top 10 most similar words to the given word in the model's vocabulary, and we used this to see how well our word2vec model worked. While the results here are not entirely random and suggest that our model does capture some semantic meaning, these results are far worse than google's pretrained word2vec model (below)."
      ],
      "metadata": {
        "id": "ihNLvTcC9xqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_w2v.wv.most_similar(\"dinner\", topn=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1ei_mfb_Bim",
        "outputId": "5918e0a8-5741-442c-c21d-0e17319874a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('lange', 0.9288738369941711), ('become', 0.9264037609100342), ('bachi', 0.9216042757034302), ('hostess', 0.918390154838562), ('marrow', 0.9169269800186157), ('update', 0.9166852235794067), ('putting', 0.914053201675415), ('bed', 0.9140514135360718), ('regular', 0.9127137660980225), ('ignored', 0.9120527505874634)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vectors.most_similar(\"dinner\", topn=10))"
      ],
      "metadata": {
        "id": "RK01a3a--cEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58b25a7-5bd9-4ec2-ce19-4808233fbaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('dinners', 0.7902063131332397), ('brunch', 0.790051281452179), ('Dinner', 0.7639397382736206), ('supper', 0.7596100568771362), ('luncheon', 0.7099571228027344), ('banquet', 0.7032414674758911), ('breakfast', 0.7007028460502625), ('buffet_dinner', 0.6914125680923462), ('meal', 0.6843624114990234), ('lunch', 0.6815704703330994)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our model, the data to be classified is a sentence, not individual words. So now that we have trained word2vec and can embed each word as a vector, we take the average of the vectors for all tokens in a sentence to get a vector representation of the entire sentence. "
      ],
      "metadata": {
        "id": "-LBZ9W4JxI-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "count = 0\n",
        "sentence_embeddings = np.zeros((3000, 100))\n",
        "num = 0\n",
        "for phrase in all_tokenized_sentences[\"Phrases\"]:\n",
        "  vec = np.zeros(100).reshape((1, 100))\n",
        "  for word in phrase:\n",
        "    try:\n",
        "        vec += model_w2v[word].reshape((1, 100))\n",
        "        count += 1\n",
        "    except KeyError:\n",
        "      continue\n",
        "    if count != 0:\n",
        "      vec /= count\n",
        "  sentence_embeddings[num]= vec\n",
        "  num += 1\n",
        "  count = 0\n",
        "\n",
        "embeddings_df = pd.DataFrame(sentence_embeddings)\n",
        "\n",
        "#we previously tried normalizing vectors\n",
        "\"\"\"\n",
        "#print(embeddings_df.mean().mean())\n",
        "#print(embeddings_df.std().std())\n",
        "\n",
        "embeddings_df1 = embeddings_df.sub(embeddings_df.mean(axis=1),axis = 'rows')\n",
        "embeddings_df2 = embeddings_df1.div(embeddings_df.std(axis=1),axis = 'rows')\n",
        "embeddings_df = embeddings_df2\n",
        "\n",
        "embeddings_df[\"Labels\"] = all_tokenized_sentences[\"Labels\"]\n",
        "embeddings_df = embeddings_df.dropna(how='any',axis=0) \n",
        "\"\"\"\n",
        "print(embeddings_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-yK8qqQA3ZZ",
        "outputId": "b1f308fc-7db1-4585-abaa-0d855dca16ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            0         1         2   ...        97        98        99\n",
            "0     0.046459  0.023194  0.000837  ... -0.027057  0.028209  0.000286\n",
            "1     0.015237  0.023584  0.009458  ... -0.017994 -0.013727 -0.017573\n",
            "2     0.017664  0.007595  0.006733  ... -0.009008  0.004559  0.004932\n",
            "3     0.083060  0.009398 -0.001145  ... -0.051467  0.027076 -0.053451\n",
            "4     0.037847  0.009748  0.012589  ... -0.012116 -0.014882  0.003329\n",
            "...        ...       ...       ...  ...       ...       ...       ...\n",
            "2995  0.032723  0.014882  0.015116  ... -0.014789 -0.010481 -0.000783\n",
            "2996  0.032856  0.053026  0.049837  ...  0.011872 -0.043454  0.019117\n",
            "2997  0.112321  0.061408  0.055865  ... -0.075427 -0.013113  0.031067\n",
            "2998  0.047290  0.017800  0.017361  ... -0.021853  0.001053  0.015029\n",
            "2999  0.042475  0.025446  0.032964  ... -0.019595 -0.051928  0.029802\n",
            "\n",
            "[3000 rows x 100 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the same process of converted word2vec word embeddings to a single vector for the entire sentence, this time for google's word2vec model. "
      ],
      "metadata": {
        "id": "1tJCbU5DApbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pt_sentence_embeddings = np.zeros((3000, 300))\n",
        "num = 0\n",
        "for phrase in all_tokenized_sentences[\"Phrases\"]:\n",
        "  vec = np.zeros(300).reshape((1, 300))\n",
        "  for word in phrase:\n",
        "    try:\n",
        "        vec += word_vectors[word].reshape((1, 300))\n",
        "        count += 1\n",
        "    except KeyError:\n",
        "      continue\n",
        "    if count != 0:\n",
        "      vec /= count\n",
        "  pt_sentence_embeddings[num]= vec\n",
        "  num += 1\n",
        "  count = 0\n",
        "\n",
        "  pt_embeddings_df = pd.DataFrame(pt_sentence_embeddings)\n"
      ],
      "metadata": {
        "id": "YQqdw4UVTiVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, same process but for the big dataset."
      ],
      "metadata": {
        "id": "Etj5Hl7iErZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bigger dataset\n",
        "\n",
        "big_embeddings = np.zeros((25000, 300))\n",
        "num = 0\n",
        "for phrase in data[\"review\"]:\n",
        "  vec = np.zeros(300).reshape((1, 300))\n",
        "  for word in phrase:\n",
        "    try:\n",
        "        vec += word_vectors[word].reshape((1, 300))\n",
        "        count += 1\n",
        "    except KeyError:\n",
        "      continue\n",
        "    if count != 0:\n",
        "      vec /= count\n",
        "  big_embeddings[num]= vec\n",
        "  num += 1\n",
        "  count = 0\n",
        "\n",
        "  big_embeddings_df = pd.DataFrame(big_embeddings)\n"
      ],
      "metadata": {
        "id": "5OOCeptRjUHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM Model\n",
        "Our first approach was to use a Support Vector Machine model for binary classification. The two classes are 1 for positive sentiment, and 0 for negative sentiment. We trained the model on the vector representations of each sentence from word2vec. We settled on a linear kernel and regularization parameter of one after doing a hyperparameter sweep with gridsearch. Below is the output of this model's f1 score, accuracy score, and the confusion matrix. The scores for this model as seen below are slightly lower than when stemming is used, because stemming adds more instances of certain words so word2vec is better on the sample review data. The accuracy with stemming for this model is about 4% higher. "
      ],
      "metadata": {
        "id": "RjkB_Gzexwyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix \n",
        "\n",
        "#embeddings_df_no_labels  = embeddings_df.loc[:,embeddings_df.columns != 'Labels']\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(embeddings_df, all_tokenized_sentences['Labels'], random_state=42, test_size=0.25)\n",
        "\n",
        "svc = svm.SVC(kernel='linear', C=1, gamma = 1, probability=True).fit(xtrain, ytrain) \n",
        "prediction = svc.predict_proba(xtest) \n",
        "prediction_int = prediction[:,1] >= 0.5\n",
        "prediction_int = prediction_int.astype(np.int) \n",
        "\n",
        "print(f1_score(ytest, prediction_int))\n",
        "print(accuracy_score(ytest, prediction_int))\n",
        "print(confusion_matrix(ytest, prediction_int))\n",
        "TN, FP, FN, TP = confusion_matrix(ytest, prediction_int).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHFVwzDNExdp",
        "outputId": "7b31cc62-84a0-4961-c888-be73c663673c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6639676113360324\n",
            "0.668\n",
            "[[255 114]\n",
            " [135 246]]\n",
            "TP = 246\n",
            "FP = 114\n",
            "FN = 135\n",
            "TN = 255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM model with google's word2vec. Interestingly, when we attempted to normalize the vectors, we got better scores with this model (5% higher accuracy). However, normalizing the vectors caused the svm model with our word embeddings to have lower accuracy. Its possible that this is because google's word embeddings contain very small numbers, so scaling them up was better for our small sample. "
      ],
      "metadata": {
        "id": "FGiKtfs1MmK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "xtrain_pt, xtest_pt, ytrain_pt, ytest_pt = train_test_split(pt_embeddings_df, all_tokenized_sentences['Labels'], random_state=42, test_size=0.25)\n",
        "\n",
        "svc_pt = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_pt, ytrain_pt) \n"
      ],
      "metadata": {
        "id": "JqnP9SfQT-pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_pt = svc_pt.predict_proba(xtest_pt) \n",
        "prediction_int_pt = prediction_pt[:,1] >= 0.5\n",
        "prediction_int_pt = prediction_int_pt.astype(np.int) \n",
        "print(f1_score(ytest_pt, prediction_int_pt))\n",
        "print(accuracy_score(ytest_pt, prediction_int_pt))\n",
        "print(confusion_matrix(ytest_pt, prediction_int_pt))\n",
        "TN, FP, FN, TP = confusion_matrix(ytest_pt, prediction_int_pt).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svR9X9_suxPn",
        "outputId": "206c5286-3fc6-4654-fb14-d2faba2b1c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6344086021505376\n",
            "0.6358768406961178\n",
            "[[239 133]\n",
            " [139 236]]\n",
            "TP = 236\n",
            "FP = 133\n",
            "FN = 139\n",
            "TN = 239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM model for big dataset, unfinished (accuracy hovered around 50% so we must either have a lingering bug or a mistaken assumption in this model)"
      ],
      "metadata": {
        "id": "nB7aUaflNQNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Big dataset svm\n",
        "big_embeddings_df_no_labels  = big_embeddings_df.loc[:,big_embeddings_df.columns != 'Labels']\n",
        "xtrain_big, xtest_big, ytrain_big, ytest_big = train_test_split(big_embeddings_df_no_labels, big_embeddings_df['Labels'], random_state=42, test_size=0.25)\n",
        "print(xtrain_big[:10])\n",
        "print(ytrain_big[:10])\n",
        "\n",
        "svc_big = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_big, ytrain_big) \n",
        "prediction_big = svc_big.predict_proba(xtest_big) \n",
        "prediction_int_big = prediction_big[:,1] >= 0.5\n",
        "prediction_int_big = prediction_int_big.astype(np.int) \n",
        "print(f1_score(ytest_big, prediction_int_big))\n",
        "print(accuracy_score(ytest_big, prediction_int_big))\n",
        "print(confusion_matrix(ytest_big, prediction_int_big))\n",
        "TN, FP, FN, TP = confusion_matrix(ytest_big, prediction_int_big).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))"
      ],
      "metadata": {
        "id": "zPSKKb6Plnt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Search\n",
        "gridsearch for best svm parameters-looking at regularization parameter, kernel type, and kernel coefficient (for rbf)"
      ],
      "metadata": {
        "id": "kv1AJkYSzU85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf','linear']}\n",
        "\n",
        "grid = GridSearchCV(svm.SVC(probability=True), param_grid, refit = True, verbose = 3)\n",
        " \n",
        "# fitting the model for grid search\n",
        "grid.fit(xtrain, ytrain)\n",
        "\n",
        "print(grid.best_params_)\n",
        " \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid.best_estimator_)"
      ],
      "metadata": {
        "id": "KQ2llezxI27S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_predictions = grid.predict_proba(xtest)\n",
        "grid_prediction_int = grid_predictions[:,1] >= 0.3\n",
        "grid_prediction_int = grid_prediction_int.astype(np.int) \n",
        "print(f1_score(ytest, grid_prediction_int))\n",
        "print(accuracy_score(ytest, grid_prediction_int))\n",
        "\n",
        "svcbest = svm.SVC(kernel='rbf', C=1000, gamma= 1, probability=True).fit(xtrain, ytrain) \n",
        "prediction2 = svcbest.predict_proba(xtest) \n",
        "prediction_int2 = prediction2[:,1] >= 0.3\n",
        "prediction_int2 = prediction_int2.astype(np.int) \n",
        "print(f1_score(ytest, prediction_int2))\n",
        "print(accuracy_score(ytest, prediction_int2))"
      ],
      "metadata": {
        "id": "O939RJq4KVhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP Model\n",
        "Multilayer Perceptron Neural Network (MLP) is a feed-forward neural network. We chose this model for its simplicity. The solver for weight optimization that we used is Limited-Memory BFGS (lmbfgs). It is an optimizer in the family of quasi-Newton methods. While BFGS method approximates the inverse of the Hessian matrix, L-BFGS maintains a history of the past several states and gradients. This allows for optimization with many variables as this method uses less memory. We tested different hidden layers options and arrived at 2 layers of 5 hidden units for the Google embeddings and 3 layers of 15 units for our embeddings. As the method did not converge with the default maximum number of iterations, we had to increase that number to 10,000.\n",
        "\n",
        "Among our methods, this one attained the highest accuracy and the best metrics overall. \n",
        "\n",
        "For the MLP classifier with embeddings from our own word2vec directly below, the accuracy is again around 3-4% lower than the best accuracy we achieved with stemming also used. "
      ],
      "metadata": {
        "id": "uYV_x5TbzZBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                 hidden_layer_sizes=(15, 3), random_state=1,max_iter=10000)\n",
        "clf.fit(xtrain, ytrain)\n",
        "pred = clf.predict(xtest)\n",
        "print(f1_score(ytest, pred))\n",
        "print(accuracy_score(ytest,pred))\n",
        "TN, FP, FN, TP = confusion_matrix(ytest, pred).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))"
      ],
      "metadata": {
        "id": "GCbOXJ_VZ2_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51f7c74-44eb-4fa0-db9d-5b16d65abc62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6790450928381963\n",
            "0.6773333333333333\n",
            "TP = 256\n",
            "FP = 117\n",
            "FN = 125\n",
            "TN = 252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP with google's word2vec embeddings"
      ],
      "metadata": {
        "id": "F-eCozQBPDFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_pt = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "                 hidden_layer_sizes=(5, 2), random_state=1,max_iter=10000)\n",
        "clf_pt.fit(xtrain_pt, ytrain_pt)\n",
        "pred_ptc = clf_pt.predict(xtest_pt)\n",
        "print(f1_score(ytest_pt, pred_ptc))\n",
        "print(accuracy_score(ytest_pt,pred_ptc))\n",
        "TN, FP, FN, TP = confusion_matrix(ytest_pt, pred_ptc).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_foMXa_-dv8v",
        "outputId": "fcc46df3-bd49-41e9-9592-f0d9415a0cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7032967032967034\n",
            "0.6746987951807228\n",
            "TP = 288\n",
            "FP = 156\n",
            "FN = 87\n",
            "TN = 216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMPARING VADER RESULTS TO BINARY LABELS\n",
        "VADER is a rules based sentiment algorithm. It can be helpful for establishing a baseline for sentiment. The most common use for VADER where it has been found to have significant success is with tweets. VADER generates a negative, neutral, and positive score for each sentence. It then computes a compound score. We can make this score binary by choosing a threshold that best divides negative and positive data. At first I seperated sentences that VADER did not give negative or positive weight to. With this specific data, VADER had a 65-70% base accuracy and could be improved to around 80%. This improvement is done by realizing that neutral answers are more likely to be negative. For the code exploring this, see the attached FinalProjectVADER jupyter notebook.\n"
      ],
      "metadata": {
        "id": "IAbbwyQ_1cLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "We found that the MLP classifier performed better than the SVM classifier for sentiment prediction. As for word embeddings, models using google's word2vec performed slightly better/equal with our word2vec when our samples were not preprocessed with stemming and stopword filtering. With these preprocessing techniques, the models using our word2vec outperfomed the ones using google's word2vec. \n",
        "\n",
        "We would like to see what changes using a larger dataset for training the word2vec model and the classifier, as we had concerns the whole time about the small size of our training data. Also, we would like to try additional preprocessing steps with our sentences, as there are many more words that do not affect sentiment that could possibly be filtered out, and even the limited filtering that we did improved model performance. "
      ],
      "metadata": {
        "id": "3h7RF-k2PIH0"
      }
    }
  ]
}